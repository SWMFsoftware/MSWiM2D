#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import datetime as dt
import warnings
warnings.filterwarnings("ignore") # suppress warnings generated by missing data

def get_l1_lookup_table(start_date,end_date,**kwargs):
    """Download hourly IMF data in HGI coordinates and store as a lookup table file.

    The output file will be stored as an ascii lookup table for use with MSWIM2D as input. The files are
    named as L1_YYYY.dat, and stored in the repository under MSWIM2D/data/L1/. The YYYY in the file name
    is selected as start_date.year+1 to account for data files beginning several months before the year
    of study.
     Note: data is downloaded in high resolution (1-minute), then averaged for each interval.

    Args:
        start_date (datetime.datetime): start time of desired IMF file
        end_date (datetime.datetime):   end time of desired IMF file
        **kwargs:
             none implemented

    Example:
        '''python
        import datetime

        start = datetime.datetime(year=2008, month=1,  day=1)
        end   = datetime.datetime(year=2009, month=12, day=31)

        # for L1 IMF lookup table
        get_l1_lookup_table(start, end, sc='A')
        '''
    """
    import os
    import numpy as np
    import datetime as dt
    import urllib
    from dateutil import rrule
    from spacepy import pycdf
    import swmfpy
    import math

    directory = '/home/tkeebler/MSWIM2D/data/L1'
    date = start_date.year+1
    lookup_file = open('{}/l1_{}.dat'.format(directory, date), 'w')

    # Required data for imf infile - all in HGI
    imf_keys = ('year','month','day','hour','minute',
                'Br','Blat','Blon', # nT
                'Ur','Ulat','Ulon', # km/s
                'n','T')            # /cc, K

    # Dictionary containing modified OMNI data
    imf_dat,avg_imf_dat = {},{}
    imf_dat['long_day'] = list() # for averaging
    for key in imf_keys:
        imf_dat[key]=list()
        avg_imf_dat[key]=list()

    # print('Getting OMNI data...')
    data = swmfpy.web.get_omni_data(time_from = start_date, time_to = end_date,
                                    original_colnames=True, resolution='high')
    # print('OMNI data successfully retrieved.')

    # Copy useful data into dictionary.
    for i, timestamp in enumerate(data['times']):
        imf_dat['year'].append(timestamp.year)
        imf_dat['month'].append(timestamp.month)
        imf_dat['day'].append(timestamp.day)
        imf_dat['hour'].append(timestamp.hour)
        imf_dat['minute'].append(0)
        imf_dat['long_day'].append(dt.datetime(timestamp.year,
                                               timestamp.month,
                                               timestamp.day,
                                               timestamp.hour))
    imf_dat['Br'] = data['Bx, nT (GSE, GSM)'] # Bx - convert vectors after averaging
    imf_dat['Blat'] = data['By, nT (GSE)']    # By
    imf_dat['Blon'] = data['Bz, nT (GSE)']    # Bz
    imf_dat['Ur'] = data['Vx Velocity, km/s, GSE']   # Ux - convert vectors after averaging
    imf_dat['Ulat'] = data['Vy Velocity, km/s, GSE'] # Uy
    imf_dat['Ulon'] = data['Vz Velocity, km/s, GSE'] # Uz
    imf_dat['n'] = data['Proton Density, n/cc']
    imf_dat['T'] = data['Temperature, K']
    # print('Data read.')

    # Average the data hourly.
    prev_i = 0
    arrayshape = np.shape(imf_dat['long_day'])[0] - 1
    for timestamp in rrule.rrule(rrule.HOURLY, dtstart=data['times'][0], until=data['times'][-1]):
        # Find bounding indices.

        if prev_i == 0: # If beginning of dataset, use binary search.
            i = int(np.floor(np.shape(data['times'])[0]/2))
            under_i = 0
            over_i = np.shape(data['times'])[0]
            while(timestamp != imf_dat['long_day'][i] or timestamp == imf_dat['long_day'][i-1]):
                if(imf_dat['long_day'][i] > timestamp):
                    over_i = i
                    i = over_i - np.floor((over_i-under_i)/2)
                elif(imf_dat['long_day'][i] < timestamp):
                    under_i = i
                    i = under_i + np.floor((over_i-under_i)/2)
                elif(timestamp == imf_dat['long_day'][i-1]):
                    i -= 1
                i = int(i)
            start_index = i

            while(timestamp == imf_dat['long_day'][i]):
                if(i == arrayshape):
                    break
                i += 1
            end_index = i - 1

        else: # If middle of dataset, begin near last entry and search linearly.
            i = prev_i
            while(timestamp != imf_dat['long_day'][i] or timestamp == imf_dat['long_day'][i-1]):
                i += 1
            start_index = i
            while (timestamp == imf_dat['long_day'][i]):
                if (i == arrayshape):
                    break
                i += 1
            end_index = i - 1

        prev_i = end_index - 3 # set starting index for next search

        # Compute averages, excluding missing data.
        avg_imf_dat['year'].append(np.average([b for b in imf_dat['year'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['month'].append(np.average([b for b in imf_dat['month'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['day'].append(np.average([b for b in imf_dat['day'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['hour'].append(np.average([b for b in imf_dat['hour'][start_index:end_index+1] if b>=0]))
        avg_imf_dat['minute'].append(0)
        avg_imf_dat['Br'].append(np.average([b for b in imf_dat['Br'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['Blat'].append(np.average([b for b in imf_dat['Blat'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['Blon'].append(np.average([b for b in imf_dat['Blon'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['Ur'].append(np.average([b for b in imf_dat['Ur'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['Ulat'].append(np.average([b for b in imf_dat['Ulat'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['Ulon'].append(np.average([b for b in imf_dat['Ulon'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['n'].append(np.average([b for b in imf_dat['n'][start_index:end_index+1] if np.isfinite(b)]))
        avg_imf_dat['T'].append(np.average([b for b in imf_dat['T'][start_index:end_index+1] if np.isfinite(b)]))

    # Create array of Datetime objects
    avg_imf_dat['datetime'] = list()
    for i,timestamp in enumerate(avg_imf_dat['year']):
        try:
            avg_imf_dat['datetime'].append(dt.datetime(int(avg_imf_dat['year'][i]),
                                                       int(avg_imf_dat['month'][i]),
                                                       int(avg_imf_dat['day'][i]),
                                                       int(avg_imf_dat['hour'][i]),
                                                       int(avg_imf_dat['minute'][i])))
        except: continue

    # Get Earth locations for lookup table.
    earth_phi = list()
    earth_data = get_earth_ephemeris(avg_imf_dat['datetime'][0],avg_imf_dat['datetime'][-1])
    for i,timestamp in enumerate(avg_imf_dat['datetime']):
        earth_i = np.where(timestamp == np.asarray(earth_data['Epoch']))[0][0]
        earth_phi.append(earth_data['heliographicLongitude'][earth_i])

    # Count number of lines in lookup table, excluding lines with missing data.
    file_numlines = 0
    for i,timestamp in enumerate(avg_imf_dat['datetime']):
        varslist  = [avg_imf_dat['Br'][i],
                     avg_imf_dat['Blat'][i],
                     avg_imf_dat['Blon'][i],
                     avg_imf_dat['Ur'][i],
                     avg_imf_dat['Ulat'][i],
                     avg_imf_dat['Ulon'][i],
                     avg_imf_dat['n'][i],
                     avg_imf_dat['T'][i]]
        if None in varslist: continue
        elif np.isnan(varslist).any(): continue
        file_numlines += 1

    # Write header for lookup table.
    lookup_file.write('OMNI hourly solar wind data for period beginning {}{:2d}{:2d}.\n'.format(
                                                                          start_date.year,
                                                                          start_date.month,
                                                                          start_date.day)) # string description
    lookup_file.write('0 0.0 1 0 9\n') # nStep, Time, nDim, nParam, nVar
    lookup_file.write('{}\n'.format(file_numlines)) # number of lines in file
    lookup_file.write('Seconds SatPhi Br Blat Blon Ur Ulat Ulon n T') # nameVar

    # Write modified data to imf file
    epoch = dt.datetime(1965, 1, 1)
    for i,timestamp in enumerate(avg_imf_dat['datetime']):
        varslist  = [avg_imf_dat['Br'][i],
                     avg_imf_dat['Blat'][i],
                     avg_imf_dat['Blon'][i],
                     avg_imf_dat['Ur'][i],
                     avg_imf_dat['Ulat'][i],
                     avg_imf_dat['Ulon'][i],
                     avg_imf_dat['n'][i],
                     avg_imf_dat['T'][i]]
        if None in varslist: continue
        elif np.isnan(varslist).any(): continue

        # Convert from GSE to HGI
        v_earth = 30 # km/s
        varslist[0] *= -1 # reverse Br
        varslist[1] *= -1 # reverse Blat (Bphi)
        varslist[3] *= -1 # reverse Ur
        varslist[4] = -1*varslist[4] + v_earth # reverse Ulat (Uphi) and add velocity of Earth

        # Write data into lookup table.
        seconds = (timestamp - epoch).total_seconds()  # convert time to seconds from epoch
        lookup_file.write('\n{} {:.3f} {:.3f} {:.3f} {:.3f} '.format(seconds, earth_phi[i],
                                                                     varslist[0],
                                                                     varslist[1],
                                                                     varslist[2]))
        lookup_file.write('{:.3f} {:.3f} {:.3f} '.format(varslist[3],
                                                         varslist[4],
                                                         varslist[5]))
        lookup_file.write('{:.3f} {:.3f}'.format(varslist[6],varslist[7]))

    lookup_file.close()

    # print('IMF lookup table file for {} created.'.format(date))
    return

def get_stereo_lookup_table(start_date,end_date,**kwargs):
    """Download hourly IMF data in HGI coordinates and store as a lookup table file.

    The output file will be stored as an ascii lookup table for use with MSWIM2D as input. The files are
    named as STEREOA_YYYY.dat, and stored in the repository under MSWIM2D/data/STEREOA/. The YYYY in the
    file name is selected as start_date.year+1 to account for data files beginning several months before
    the year of study.

    Args:
        start_date (datetime.datetime): start time of desired IMF file
        end_date (datetime.datetime):   end time of desired IMF file
        **kwargs:
             sc (str): (default: 'A') case insensitive selection of desired spacecraft
                                      choose 'A' for STEREO-A
                                      choose 'B' for STEREO-B

    Example:
        '''python
        import datetime

        start = datetime.datetime(year=2008, month=1,  day=1)
        end   = datetime.datetime(year=2009, month=12, day=31)

        # for STEREO-A lookup table
        get_stereo_lookup_table(start, end, sc='A')

        # for STEREO-B lookup table
        get_stereo_lookup_table(start, end, sc='B')
        '''
    """
    import os
    import numpy as np
    import datetime as dt
    import urllib.request
    from dateutil import rrule
    from spacepy import pycdf

    # Select STEREO-A or STEREO-B
    spacecraft = 'A'
    if 'sc' in kwargs:
        spacecraft = kwargs['sc'].upper()

    # Change data directory to match spacecraft.
    if spacecraft=='A':
        data_url = 'https://spdf.gsfc.nasa.gov/pub/data/stereo/ahead/coho1hr_magplasma/'
    elif spacecraft=='B':
        data_url = 'https://spdf.gsfc.nasa.gov/pub/data/stereo/behind/coho1hr_magplasma/'
    else:
        print('Invalid request. Please select sc=\'A\' or sc=\'B\' in **kwargs.')
        return

    # Check input ranges.
    if (start_date < dt.datetime(2007,1,1) or end_date > dt.datetime(2019,12,1)) and spacecraft == 'A':
        print('Error: Date range must be between 2007.01.01 and 2019.12.01 for STEREO-A.')
        return
    elif (start_date < dt.datetime(2007,1,1) or end_date > dt.datetime(2014,9,1)) and spacecraft == 'B':
        print('Error: Date range must be between 2007.01.01 and 2014.09.01 for STEREO-B.')
        return

    # Set directory to save data.
    directory = '/home/tkeebler/MSWIM2D/data/STEREO{}'.format(spacecraft) ## UPDATE THIS LINE
    if start_date.year == 2007: dirdate = start_date.year
    else: dirdate = start_date.year + 1

    stereo_keys = ['Epoch',
                   'radialDistance','heliographicLatitude','heliographicLongitude', # in HGI: [AU],[deg],[deg]
                   'BR','BT','BN','B', # in RTN: [nT],[nT],[nT],[nT]
                   'plasmaSpeed','lat','lon', # SW lat, lon angles in RTN coordinate system: [km/s],[deg],[deg]
                   'plasmaDensity','plasmaTemp'] # [n/cc],[K]

    # Required data for imf infile.
    imf_keys = ('datetime','SatPhi',# [seconds from epoch], [deg]
                'Br','Blat','Blon', # [nT]
                'Ur','Ulat','Ulon', # [km/s]
                'n','T')            # [n/cc], [K]

    # Dictionary containing modified STEREO data.
    imf_data = {}
    for key in imf_keys:
        imf_data[key]=list()

    # Download data in monthly intervals from GSFC CDAWeb.
    print('Getting STEREO-{} data...'.format(spacecraft))
    stereo_data = {}
    for key in stereo_keys:
        stereo_data[key] = list()
    for date in rrule.rrule(rrule.MONTHLY,dtstart=start_date,until=end_date):
        if spacecraft=='A':
            data_filename = '{0}/sta_coho1hr_merged_mag_plasma_{0}{1:02d}01_v01.cdf'.format(date.year,date.month)
        else:
            data_filename = '{0}/stb_coho1hr_merged_mag_plasma_{0}{1:02d}01_v01.cdf'.format(date.year,date.month)
        urllib.request.urlretrieve(data_url+data_filename,'stereo.cdf')
        stereo_cdf = pycdf.CDF('stereo.cdf')
        for key in stereo_keys:
            stereo_data[key].extend(stereo_cdf[key])
        stereo_cdf.close()
        os.remove('stereo.cdf')
    print('STEREO-{} data successfully retrieved.'.format(spacecraft))

    # Set missing values to numpy.nan
    for key in stereo_keys:
        if key == 'Epoch': continue
        for i, entry in enumerate(stereo_data[key]):
            if entry < -1e28:
                stereo_data[key][i] = np.nan

    for i, timestamp in enumerate(stereo_data['Epoch']):
        data_list = [timestamp,
                     stereo_data['heliographicLongitude'][i],
                     stereo_data['BR'][i],stereo_data['BT'][i],
                     stereo_data['BN'][i],stereo_data['plasmaSpeed'][i],
                     stereo_data['lat'][i],stereo_data['lon'][i],
                     stereo_data['plasmaDensity'][i],stereo_data['plasmaTemp'][i]]
        if np.any(np.isnan(data_list[1:])):continue
        imf_data['datetime'].append(data_list[0])
        imf_data['SatPhi'].append(data_list[1])
        imf_data['Br'].append(data_list[2])
        imf_data['Blat'].append(data_list[3])
        imf_data['Blon'].append(data_list[4])
        speed = data_list[5]
        lat = np.radians(data_list[6])
        lon = np.radians(data_list[7])
        imf_data['Ur'].append(speed*np.cos(lat)*np.cos(lon))
        imf_data['Ulat'].append(speed*np.cos(lat)*np.sin(lon))
        imf_data['Ulon'].append(speed*np.cos(lon)*np.sin(lat))
        imf_data['n'].append(data_list[8])
        imf_data['T'].append(data_list[9])

    # Create lookup table.
    lookup_file = open('{}/STEREO{}_{}.dat'.format(directory,spacecraft,dirdate), 'w')
    file_numlines = np.shape(imf_data['datetime'])[0]

    # Write header for lookup table.
    lookup_file.write('STEREO-{} hourly solar wind data for period beginning {}.\n'.format(spacecraft,dirdate))
    lookup_file.write('0 0.0 1 0 9\n') # nStep, Time, nDim, nParam, nVar
    lookup_file.write('{}\n'.format(file_numlines)) # number of lines in file
    lookup_file.write('Seconds SatPhi Br Blat Blon Ur Ulat Ulon n T') # nameVar

    # Write data to lookup table.
    imf_data['datetime'] = np.asarray(imf_data['datetime'])
    epoch = dt.datetime(1965, 1, 1)
    for i, timestamp in enumerate(imf_data['datetime']):
        seconds = (timestamp - epoch).total_seconds()  # convert time to seconds from epoch
        lookup_file.write('\n{} {:.2f} {:.2f} {:.2f} {:.2f} '.format(seconds, imf_data['SatPhi'][i],
                                                                     imf_data['Br'][i],
                                                                     imf_data['Blat'][i],
                                                                     imf_data['Blon'][i]))
        lookup_file.write('{:.2f} {:.2f} {:.2f} '.format(imf_data['Ur'][i],
                                                         imf_data['Ulat'][i],
                                                         imf_data['Ulon'][i]))
        lookup_file.write('{:.2f} {:.2f}'.format(imf_data['n'][i],
                                                 imf_data['T'][i]))
    lookup_file.close()

    #print('STEREO-{} lookup table for {} created.'.format(spacecraft,dirdate))
    return

def get_earth_ephemeris(start_date,end_date,**kwargs):
    """
    Args:
       start_date (datetime.datetime): start time from IMF data
        end_date (datetime.datetime):   end time from IMF data
       **kwargs: none implemented
    :return:
    """
    import os
    import numpy as np
    import urllib.request
    from dateutil import rrule
    from spacepy import pycdf

    directory = '/home/tkeebler/MSWIM2D/'
    dirdate = '{}{:02d}{:02d}'.format(start_date.year,
                                      start_date.month,
                                      start_date.day)

    earth_keys = ['Epoch',
                  'heliographicLatitude', 'heliographicLongitude',  # [AU],[deg HGI],[deg HGI]
                  'azimuthAngle', 'elevAngle',
                  'ABS_B', 'BN', 'BR', 'BT',
                  'N', 'T', 'V']
    earth_data = {}
    for key in earth_keys:
        earth_data[key] = list()
    data_url = 'https://cdaweb.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/coho1hr_magplasma/'

    # print('Accessing Earth ephemeris...')
    for date in rrule.rrule(rrule.MONTHLY, dtstart=start_date, until=end_date):
        data_filename = '{}/omni_coho1hr_merged_mag_plasma_{}{:02d}01_v01.cdf'.format(date.year, date.year,
                                                                                    date.month)
        urllib.request.urlretrieve(data_url + data_filename, 'earth_ephemeris.cdf')
        earth_cdf = pycdf.CDF(directory + 'earth_ephemeris.cdf')

        for key in earth_keys:
            earth_data[key].extend(earth_cdf[key])
        earth_cdf.close()
        os.remove('earth_ephemeris.cdf')

    # print('Earth ephemeris data for {} accessed.'.format(dirdate))
    return earth_data


# Make all L1 input lookup tables.
#    valid for [1985, 2019]
#    Note: usable data coverage begins in 1995
for y in np.arange(1995, 2020, 1):
    get_l1_lookup_table(dt.datetime(year=y-1, month=11, day=1),
                        dt.datetime(year=y+1, month=1, day=31))
    print('########## L1_{} complete.#############'.format(y))

# Make all STEREOA input lookup tables.
#    valid for [2007, 2019]
for y in np.arange(2007, 2020, 1):
    if y == 2007:
        startmonth = 1
        startyear = y
    else:
        startmonth = 11
        startyear = y-1
    if y == 2019:
        endmonth = 12
        endday = 1
        endyear = y
    else:
        endmonth = 1
        endday = 31
        endyear = y+1
    get_stereo_lookup_table(dt.datetime(year=startyear, month=startmonth, day=1),
                            dt.datetime(year=endyear, month=endmonth, day=endday),
                            sc='A')
    print('########## STEREOA_{} complete.#############'.format(y))
    
# Make all STEREOB input lookup tables.
#    valid for [2007, 2014]
for y in np.arange(2007, 2015, 1):
    if y == 2007:
        startmonth = 1
        startyear = y
    else:
        startmonth = 11
        startyear = y-1
    if y == 2014:
        endmonth = 9
        endday = 1
        endyear = y
    else:
        endmonth = 1
        endday = 31
        endyear = y+1
    get_stereo_lookup_table(dt.datetime(year=startyear, month=startmonth, day=1),
                        dt.datetime(year=endyear, month=endmonth, day=endday),
                        Uniform=False, sc='B')
    print('########## STEREOB_{} complete.#############'.format(y))
